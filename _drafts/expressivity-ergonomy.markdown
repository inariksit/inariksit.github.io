---
layout: post
title:  "Expressivity and ergonomy of GF"
date:   2024-05-??
categories: gf
---

If you are reading this blog, you may have asked yourself the following question:

**Should I write my grammar in GF, or would I be fine with a more mainstream but less expressive formalism?**

One of GF's selling points is its superior expressivity compared to its alternatives (that are often *context-free grammars*), but the standard explanations for that sort of expressivity are either combinations of symbols that don't look even remotely like natural language, or Swiss German triple subordinate clauses about promising to help someone paint their house. If neither of these sounds like your use case, is it worthwhile to learn GF?

In the first part of this post, I will first explain the concept of **expressivity**, focusing specifically on the spot that is interesting for natural language purposes.
In the second part of the post, I approach the question from the point of view of **ergonomy**.
I argue that even if it is *possible* to express all of your domain with another tool, it might still be a more *pleasant* experience in GF.


<!-- The takeaway from this post will be as follows: Even if it is possible express all of your domain with another tool, it might still be a more pleasant experience in GF. -->




<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [Part I: Expressivity of GF](#part-i-expressivity-of-gf)
  - [What is expressivity: a gentle introduction](#what-is-expressivity-a-gentle-introduction)
    - [The Chomsky hierarchy: a high-level overview](#the-chomsky-hierarchy-a-high-level-overview)
    - [Regular languages](#regular-languages)
    - [Context-free languages](#context-free-languages)
    - [Context-sensitive languages](#context-sensitive-languages)
    - [Recursively enumerable languages](#recursively-enumerable-languages)
  - [Where is natural language](#where-is-natural-language)
  - [Where is GF](#where-is-gf)
- [Part II: Ergonomy of GF](#part-ii-ergonomy-of-gf)
  - [Actually abstract ASTs](#actually-abstract-asts)
    - [Inflection tables, or ‚Äúwhose subtree am I?‚Äù](#inflection-tables-or-whose-subtree-am-i)
    - [Inherent parameters, or ‚Äúwho is my subtree?‚Äù](#inherent-parameters-or-who-is-my-subtree)
  - [Making illegal states unrepresentable](#making-illegal-states-unrepresentable)
    - [Meaning of parameters](#meaning-of-parameters)
    - [Banning trees vs. never creating them](#banning-trees-vs-never-creating-them)
  - [When to do tree transformations](#when-to-do-tree-transformations)
- [Footnotes](#footnotes)

<!-- /code_chunk_output -->



## Part I: Expressivity of GF

It's not essential to understand every single detail of this section to appreciate the section about ergonomy. I start with a brief handwavy introduction for those who are new to formal language theory, followed by a more exact definition for those who aren't.

If you're looking for a rigorous treatise full of formal language theory, you're just one click away! Such a monograph has existed for 20 years already, so go ahead and read [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) *"Expressivity and Complexity of Grammatical Framework"*, if that's what you're after. You may also want to skip the whole gentle introduction, and jump directly to [Where is natural language](#where-is-natural-language).

### What is expressivity: a gentle introduction

In this post, and in formal language theory in general, "language" means a set of strings generated by some grammar. We call these *generative grammars*, and they work so that you define set of rules, and by following the rules, you end up with some string.

Here's a verbal description of a trivial grammar, which generates the language `{dog barks, cat barks, dog meows, cat meows}`.

> **Rule 1.** Concatenate one noun and one verb (in that order).
> **Rule 2.** Nouns are: `{dog, cat}`.
> **Rule 3.** Verbs are: `{meows, barks}`.

There are different ways of writing grammars, and with some of them, you can describe more complex phenomena than others. (What counts as a "complex phenomenon" in formal language theory doesn't necessarily correspond to a language learner's intuitions of what is difficult!)
The different kinds of grammars are divided into a [*hierarchy*](https://en.wikipedia.org/wiki/Chomsky_hierarchy), named after its inventor Noam Chomsky.
 <!-- based on what kinds of languages they can express. -->


#### The Chomsky hierarchy: a high-level overview

The picture below is intended to give a rough idea about the landscape. Don't worry if you don't understand every box, or even most of them. (The natural language bit is supposed to be "between categories"‚ÄîI'll get to that later!)

<!-- If you are already familiar with the Chomsky hierarchy, you can jump ahead to [Where is natural language](#where-is-natural-language). -->

![Simplified summary of Chomsky hierarchy and language complexity](/images/chomsky-hierarchy-handwavy-explanation.png "Simplified summary of Chomsky hierarchy and language complexity")

Now I'm going to briefly explain each level. Whenever I introduce a feature that demonstrates how one level is more expressive than the one before it, you may assume that the new feature is a *representative example*, but far from *exhaustive*.

#### Regular languages

At the simplest level, we have **regular languages**. Our example string in the picture is the humble "aaaa‚Ä¶", which belongs to the language "any number of letter *a*".

What makes this language so simple? It's not because we only deal with a single character! We could've specified "any number of any Unicode characters in any order", and that language would've been a regular language‚Äîequally simple as our strings of plain *a*'s.

The simplicity comes from its (lack of) *tree structure* and (lack of) information about *what's happening elsewhere*.
<!--
 Here we have languages like "any number of letter *a*" or even "any number of *a*' followed by any number of *b*". But if you wanted to specify anything like "the same amount of *a* and *b*", that would be impossible: there is no way to remember what has come before the very latest token. -->

![A tree for the string 'aaaaaaaaaa'.](/images/aaaa.png "A tree for the string 'aaaaaaaaaa'.")

This tree is completely linear. The letters just come one after another, there is no more sophisticated structure. Furthermore, each letter in the string knows only what letter came immediately before it. This means that you would **not** be able to express anything that needs a longer "memory" (not a technical term), such as ~~"any amount of letter *a*, followed by the same amount of letter *b*"~~.

#### Context-free languages

Next in the hierarchy are **context-free languages**. Typical examples are markup languages or programming languages.

Compared to the regular languages, these languages have a slightly longer "memory". The HTML fragment in the [high-level overview](#the-chomsky-hierarchy-a-simplified-overview) is a good example: the `<a>` tag must be terminated with a corresponding `</a>` tag, and the complete `<a>‚Ä¶</a>` pair must be fully enclosed in the `<div> ‚Ä¶ </div>` tags.

For the sake of illustration, let's take the language that was impossible with regular grammars: "**any amount** of letter *a*, followed by **the same amount** of letter *b*". The conventional notation for this language is `a‚Åøb‚Åø`, which means "*n* repetitions of each letter."

Two things have changed from the sad-looking linear "trees" in the previous section.

![A tree for the string 'aaabbb' from the language a‚Åøb‚Åø](/images/aaabbb.png "A tree for the string 'aaabbb' from the language a‚Åøb‚Åø")

1. The trees have a non-linear, *nested* tree structure. (Intuition: they look more like üå≤ or üåø! Multiple things can grow from them simultaneously.) <!-- Real plants like üå≤ or üåø can grow new leaves in multiple places simultaneously, and so do context-free grammars. -->
2. In order to only produce strings that have exactly the right amounts of *a* and *b*, they need to come in pairs. You could replace any pair of (*a*,*b*) in the tree with a pair of opening and closing HTML tags. ![Nested HTML](/images/html.png "Nested HTML")

<!-- To allow for a larger lexicon (or *alphabet* as it's called in formal language theory), the grammar just needs more rules, but their idea is the same. Things can come in pairs and be nested

how many different characters we're dealing with is irrelevant! the full HTML specification has lots of different tags, but it's equally simple -->

However, if we wanted agreement between *more than two* elements, such as `a‚Åøb‚Åøc‚Åø`, that would no longer be a context-free language. Luckily, we have more levels of the Chomsky hierarchy!
<!-- (More on this language in the next section.) -->

<!-- We could pump out an "abc", and then surround it by more letters, but there's no way to insert those  -->

#### Context-sensitive languages

The next level is **context-sensitive languages**. There is no "typical example" of a context-sensitive language that would be familiar to non-experts. Contrast how HTML, an example of a context-free language, is known by many people who have never heard about the Chomsky hierarchy! But context-sensitive languages as a class don't have such a famous representative.

Previously, we saw that the language `a‚Åøb‚Åø` was context-free, and `a‚Åøb‚Åøc‚Åø` was not. But now we're in a more expressive realm, so we can generate that language! Here's a tree for the string "aabbcc".

![A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø](/images/aabbcc.png "A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø")

Look at that second *b* sneaking in between two letters! This would be impossible to do at a lower level. In a context-free language, you are allowed to output multiple letters at a time, but the moment you have laid them out on the paper, *their order may not change*, you can only add more letters before and after. In contrast, context-sensitive languages allow this kind of "late edits" (not a technical term).

<!-- They allow lots of other things too, but I want to keep this section short‚Äîthis is just to give some intuition. -->

I have a few things to say about natural language, but I'm saving it for the section [Where is natural language](#where-is-natural-language). So let's just finish climbing up the Chomsky hierarchy‚Äîonly one more level to go!

#### Recursively enumerable languages

The highest level is called **recursively enumerable languages**. Here I have used the set of prime numbers as an example language. This might look strange from a linguistic perspective: how come a list of numbers is "more complex" than a piece of legalese jargon?

The answer is that the grammar that generates the set of prime numbers has to actually *compute* the set. So it's not really a "grammar" in the same way as a book describing the morphosyntax of Basque or Greenlandic is a grammar. In formal language theory, grammars are actually *models of computation*.

The simpler ~~grammars~~ models of computation just deal with strings, but once we get to the top of the hierarchy, they can do whatever a Turing machine can. (If you don't know what a Turing machine is, just mentally replace it with "a computer".) As long as the answer to some computational problem has a textual representation, we can define a corresponding language to be *the set of all and only the strings that answer the problem*.


| Language | Grammar / Model of computation |
|----------|----------------------------------|
| All prime numbers | A Turing machine that computes prime numbers |
| (Singleton set of) the string that spells out the answer to life, universe and everything |  [Deep Thought](https://hitchhikers.fandom.com/wiki/Deep_Thought) |

If you thought this was interesting, you're lucky because there's a whole world[^8] out there, way beyond this blog and natural-language-oriented grammar formalisms!

If you thought this was a bunch of nonsense, you're lucky because ~~it won't be on the test~~ I have never seen any useful application of recursively enumerable languages in computational linguistics.

Speaking of which, now is finally the time to talk about natural language.

### Where is natural language

Computational linguists ignore most of the Chomsky hierarchy, and zoom in on a tiny dot, right at the border of context-free and context-sensitive languages. The image below is but a gross oversimplification of the whole ecosystem.
<!-- The niche is filled with obscure grammar formalisms, of which only a lucky few have ever been mentioned on Stack Overflow. -->


![Chomsky hierarchy, zoomed in between context-free and context-sensitive.](/images/chomsky-hierarchy-zoomed-in.png "Chomsky hierarchy, zoomed in between context-free and context-sensitive.")

<font size="2">*Not relevant for the post, but if you want to know what the abbreviations stand for, check the footnote.[^9]*</font>

Many linguists agree that natural language goes beyond context-free[^1], but doesn't quite need the full power of context-sensitive languages. So people have set out to discover *subclasses* right between the two classes, each with different pros and cons. For these researchers, the tradeoff is expressivity of the formalism vs. a fast parsing algorithm.

But a lot of people still use plain old context-free grammars (CFGs) in order to model some *fragment* of natural language! This is a reasonable engineering decision, if you know that you only need to express a very simple domain. Context-free grammars are relatively fast, well-known, easy to use and easy to find answers on Stack Overflow (especially compared to any of the formalisms named in the picture!) For a developer, the tradeoff is expressivity of the formalism vs. the learning curve to actually become productive.


<!-- The relevant landscape for natural languages is somewhere around context-free and context-sensitive.
![Chomsky hierarchy, only CF and CS](/images/chomsky-hierarchy-handwavy-explanation-cf-cs.png "Simplified summary of Chomsky hierarchy and language complexity, only focusing on natural language") -->

<!--
There is no scientific consensus on the classification of natural languages in the Chomsky hierarchy.

Even if there was one, it would still leave a lot of engineering questions,.

But for engineering purposes, there are plenty of empirical answers! There are different grammar formalisms with different features, and we can try them out to implement our applications.



  - Most grammar formalisms with somewhat mainstream adoption are somewhere between context-free (CF) and context-sensitive (CS).
  - In fact, formal language theorists have made up lots of subdivisions between the 4 classes! This is useful for performance purposes. The jump from CF to CS is pretty significant, and there are some things that are less relevant
  - For engineering purposes, there are plenty of usable fragments that can be expressed with context-free grammars.
- Some phenomena in some natural languages [aren't context-free](https://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf).
 -->



### Where is GF

GF is [equivalent to Parallel Multiple Context-Free Grammar](../../06/13/pmcfg.html#terminology) (PMCFG). Its expressive power is between *mildly context-sensitive*[^4] and *context-sensitive* languages, in a class so tiny that it doesn't have an established name. This means that GF can express a *subset* of context-sensitive languages, but not all of them. However, it's a very useful subset for natural language purposes.

So let's demonstrate GF's expressive power by writing a GF grammar for the context-sensitive language `a‚Åøb‚Åøc‚Åø`. (The standard natural language examples are not nearly as compact.)
This is a simple task, thanks to the lincats being *tuples of strings* instead of just strings.

We start with the abstract syntax:

> ```haskell
> abstract ABC = {
>   flags startcat = S ;
>   cat
>     S ; ABC ;
>   fun
>     concatenate : ABC -> S ; -- concat As, Bs, Cs into a single string
>     increment : ABC -> ABC ; -- increment counts of A, B, C
>     empty : ABC ; -- n = 0, i.e. the empty string
> }
> ```

And here is the concrete syntax for the grammar.

> ```haskell
> concrete ABCcnc of ABC = {
>   lincat
>     S = Str ;
>     ABC = {a,b,c : Str} ;
>   lin
>     concatenate abc = abc.a ++ abc.b ++ abc.c ;
>     increment abc = {
>       a = abc.a ++ "a" ;
>       b = abc.b ++ "b" ;
>       c = abc.c ++ "c"
>       } ;
>     empty = {a,b,c = ""} ;
> }
> ```

You can test the grammar in your GF shell as follows:

```python
$ gf ABCcnc.gf
‚Ä¶
ABC> p "a a a b b b c c c"
concatenate (increment (increment (increment empty)))

ABC> p "a a b b c c" | vp -showfun -view=open
```

![GF parse tree for the string aabbcc](/images/anbncn.png "GF parse tree for the string aabbcc")

This concludes the part about expressivity. If you want to learn <!--about the larger context and how GF compares to some other grammar formalisms--> more, here's the link to [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) again. If you don't, that's cool‚Äîthis overview is more than enough to understand the rest of this post!


<!-- You can also do stuff like erase[^1] and duplicate arguments on the right-hand side. If you are familiar with the [PGF format](../../06/13/pmcfg.html) and want to explore how this works on the lower level, you can add these functions to the previous grammars:

```haskell
fun
  eraseABC : ABC -> ABC ;
  duplicateABC : ABC -> ABC ;

lin
  eraseABC _ = {a,b,c = ""} ;
  duplicateABC abc = {
    a = abc.a ++ abc.a;
    b = abc.b ++ abc.b ;
    c = abc.c ++ abc.c
    } ;
```
and while in GF shell, you can print out the PGF with the command `pg` and observe the productions and sequences. But this is not at all necessary to follow the rest of this post‚ÄîI'm just leaving the option for anyone who might be interested in exploring more. And if you just now got interested in formal language theory and GF, here's the link to [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) again.
-->


## Part II: Ergonomy of GF

As we learned in the previous section, GF being equivalent to PMCFG allows you to do some stuff that is literally impossible with a plain CFG.

But for most purposes, the relevant question to ask isn't "is it possible to represent `__` with `__`". Often the answer is yes[^2], but it's still not a good idea.

1. There are plenty of things that are perfectly *possible*, but they are *painful* when all you have is a CFG.
2. It might not be *that* painful, but you want an AST that is robust for minor changes.
3. You want multiple languages to have the same ASTs.

I feel like number 3 is the most commonly pitched thing about GF. But points 1 and 2 are relevant even if you only want to work on one language, even if that language is English, and even if you aren't using the Resource Grammar Library.[^3]

So now I will discuss how GF has [actually abstract ASTs](#actually-abstract-asts), how it [makes illegal states unrepresentable](#making-illegal-states-unrepresentable) and how ‚Ä¶ (TODO more examples?).

### Actually abstract ASTs

<!-- Coming from GF-land, it shocks me what kind of things people call ASTs. I see people writing grammars where *inflection forms of the same lexeme are in different functions*. üôÄ -->

Consider the following two sentences:

- The company <u>raises capital</u>
- Equity financing means a transaction with the purpose of <u>raising capital</u>

The underlined fragments have different grammatical functions in their respective sentences. But semantically they express the same thing, and I would expect that to be reflected in the AST of an *application grammar*.

I have a small GF grammar that generates these sentences [here](https://gist.github.com/inariksit/28e182bd4f6881cd69eb96121f048829). Now look at the highlighted subtrees in the following trees:

- The company <u>raises capital</u> ![GF tree for 'the company {raises capital}'](/images/predication.png "GF tree for 'the company raises capital'")

- (Equity financing means a) transaction with the purpose of <u>raising capital</u>![GF tree for 'equity financing means a transaction with the purpose of {raising capital}'](/images/modification.png "GF tree for 'equity financing means a transaction with the purpose of raising capital'")

The subtree `Raise (IndefItem Capital)` has a different linearization depending on its context. When it's the main predicate, it linearizes to the finite verb form "raises capital". When it's modifying a noun as a part of an adverbial, it linearizes to a gerund "raising capital".

If you've ever written context-free grammars, I hope you can appreciate how much prettier this is to the alternative, where "raises" and "raising" would be completely disconnected productions!

The properties of GF that make it *possible* to implement a‚Åøb‚Åøc‚Åø are the same that make it *pretty* to implement this grammar. Let's discuss them one at a time.

#### Inflection tables, or ‚Äúwhose subtree am I?‚Äù

Previously, we saw that the lincat of `ABC` contained 3 different string fields, where we accumulated the appropriate amount of "a", "b" and "c" to be put together later. Now we make use of the concept "lincats may be tuples of strings" to create an **inflection table**.
<!-- The full grammar is [here](https://gist.github.com/inariksit/28e182bd4f6881cd69eb96121f048829). -->

1. The lincat of `Action` consists of the fields called `pred` and `mod`, for predication and modification.

    >  ```haskell
    >  lincat
    >    Action = {pred, mod : Str} ;
    >  ```

2. The function `Raise`, which creates an `Action`, puts the appropriate forms in the fields.

    > ```haskell
    > -- : Item -> Action ;
    > Raise item = {
    >   pred = "raises" ++ item.s ; -- raises capital
    >   mod = "raising" ++ item.s ; -- raising capital
    >   } ;
    > ```

3. The functions `ActionSentence` and `WithPurpose` each choose the field they need.

    > ```haskell
    > -- Create a sentence
    > -- : Item -> Action -> S ;
    > ActionSentence item action = {
    >   s = item.s ++ action.pred
    > } ;
    >
    > -- Modify a Kind with an Action
    > -- : Action -> Kind -> Kind ;
    > WithPurpose action kind = kind ** {
    >   s = kind.s ++ "with the purpose of"
    >              ++ action.mod
    > } ;
    > ```

And that's how the same AST outputs different strings, *depending on whose subtree it is.*

If you want to practice the concept, click the footnote[^5] for some homework! Otherwise, keep reading.

#### Inherent parameters, or ‚Äúwho is my subtree?‚Äù

Let's look at this sentence again.

- (Equity financing means) <u>a transaction</u> with the purpose of raising <u>capital</u>
  ![GF tree for '{a transaction} with a purpose of raising {capital}'](/images/count_mass_nouns.png "GF tree for 'a transaction with a purpose of raising capital'")

<!--
```python
Startups> l IndefItem Transaction
a transaction

Startups> l IndefItem Capital
capital
```
We apply the same function `IndefItem` to two different `Kind`s, and get a different strategy: "a transaction" vs. "capital" without an article.
-->
We notice that the function `IndefItem` is applied twice, but the indefinite article "a" is only produced once.

This is possible, because GF has the concept of **parameter**: a finite set of user-defined values. In addition to text fields, a lincat may also contain any number of these parameters.

  <!-- As the name suggests, the parameter specifies whether the Kind is a mass noun or a count noun. -->
1. In the grammar, I have defined a parameter called `MassOrCount`. I put it in the lincat of `Kind`, in a field called `c`. (GF programmers usually say "Kind has an inherent MassOrCount parameter".)
    > ```haskell
    > param
    >   MassOrCount = Mass | Count ;
    > lincat
    >   Kind = {s : Str ; c : MassOrCount} ;
    >   Item = {s : Str} ;
    > ```

2. Each actual `Kind` has some actual `MassOrCount` value in its `c` field. <!--Capital has `Mass`, Transaction has `Count`.-->
    >  ```haskell
    >  lin
    >    Capital     = {s = "capital"     ; c = Mass} ;
    >    Transaction = {s = "transaction" ; c = Count} ;
    > ```

3. The function `IndefItem` checks its argument's `MassOrCount` value, inserting an indefinite article for count nouns and nothing for mass nouns.<!-- [^6] -->

    >  ```haskell
    >  lin
    >   -- : Kind -> Item ;
    >   IndefItem kind = {s = article ++ kind.s}
    >     where {
    >       article : Str = case kind.c of {
    >        Count => "a" ; -- simplified: a/an handled in RGL
    >        Mass  => ""  } -- no article for mass nouns
    >     } ;
    >  ```

<!-- ```python
Startups> p -cat=Item "transaction"
The parser failed at token 1: "transaction"
``` -->

This is how a function outputs different strings, *depending on which subtree it gets as an argument*.

By the way, this string "a", which is inserted not in a leaf, but by a function that takes arguments? If you hang out with the GF folks, we call such strings *syncategorematic*.
![bananagrams tiles spelling a bunch of words related to GF and similar things](/images/syncategorematicity.png "we can't even spell syncategorematicity correctly")

<!--
These two  demonstrate how GF's separation of abstract and concrete syntax, as well as lincats being tuples of strings, allows the ASTs to abstract away from such minor details as inflection forms.

Now I'm going to continue with the same example grammar, and get a bit more general still. -->

### Making illegal states unrepresentable

We have seen two features of GF that allow us to create actually abstract ASTs: inflection tables and inherent parameters. (Three features really, counting the whole *separation of abstract and concrete syntax* as the first feature!)

But what do these superior ASTs gain us? I argue that they [make illegal states unrepresentable](https://www.google.com/search?q=make+illegal+states+unrepresentable)‚Äîa popular design goal all over the programming world. So let's take another look at the parameters.

#### Meaning of parameters

The meaning of a parameter is defined individually for each function. For `IndefItem`, it looks like the following:

<!-- `MassOrCount` makes a difference for those functions that quantify a `Kind` into an `Item`.  -->

- The *semantics* of `IndefItem` is "quantify a Kind into an indefinite Item".
<!-- - The *syntactic structure*‚Äîwhether to output an article or not‚Äîdepends on the value of the Kind's `MassOrCount` parameter. -->
- The *strategy to express this*‚Äîwhether to output an article or not‚Äîdepends on the Kind's `MassOrCount` value.

This makes our grammar never generate an `Item` called ~~"a capital"~~ or ~~"transaction"~~.

But these strings are not categorically banned in our grammar‚Äîthey just aren't valid linearizations in the category of `Item`!
For example, we may decide that a term to be defined always appears without an article.

<!-- In our example, `MassOrCount` makes a difference in : it never produces the combinations ~~"a capital"~~ or ~~"transaction"~~.  -->

<!-- For each new function, we can decide how it handles mass and count nouns.

```python
# MultipleItem : Kind -> Item   (not in the current grammar)
Startups> p "many transactions"
MultipleItem Transaction

Startups> p "a lot of capital"
MultipleItem Capital
``` -->

<!-- But another function can treat the parameter differently. -->

<!-- In some context, we may decide to treat all Kinds the same way! -->
<!-- Consider these two, rather tautological, sentences. -->

```python
# DefinitionSentence : Kind -> Item -> S
Startups> p "transaction means a transaction"
DefinitionSentence Transaction (IndefItem Transaction)

Startups> p "capital means capital"
DefinitionSentence Capital (IndefItem Capital)
```

The secret technique is to completely ignore the `MassOrCount` parameter in the term to be defined.
<!-- Whether the definition (`Item`) has an article, depends on t but that depends on when it became an Item. -->
<!-- (Look at that code with its total lack of <code><span class="token keyword keyword-case">case</span> <span class="token hvariable">kind</span><span class="token punctuation">.</span><span class="token hvariable">c</span> <span class="token keyword keyword-of">of</span> ‚Ä¶</code>!) -->

> ```haskell
> lin
>  -- : Kind -> Item -> S ;
>  DefinitionSentence term defn = {
>    s = term.s  -- Kind : always without article
>     ++ "means"
>     ++ defn.s  -- Item : may have article from before
>  } ;           --      , but we don't know it here!
> ```

<!-- This may seem like a trivial example, but I just wanted to highlight how the internal parameters aren't immutable destiny. A count noun may appear without an article, if -->


#### Banning trees vs. never creating them

This is a more powerful abstraction than providing the structures for "output an article" and "output no article" in the abstract syntax, and banning certain ASTs as an afterthought.

For mass nouns, there is no need to explicitly ban the combination ~~"a capital"~~, because there is no tree that creates it in the first place.
For count nouns, both "transaction" and "a transaction" exist, each in only the desired context(s).

Some ways to ban trees: [cause a runtime exception](../../../2018/08/28/gf-gotchas.html#raise-an-exception) (weird/incorrect trees are generated, but never linearized) or implement the restrictions in an [external program](../../../2019/12/12/embedding-grammars.html). But if your application grammar is abstract enough that it only produces trees that make sense, then these techniques are unnecessary.

<!-- Anyway, the point is that "transaction" being a count noun doesn't mean something fixed, like "always appears with a determiner". It means a different thing for every function that takes it as an argument. -->

<!-- force it to always appear with a determiner. Some functions treat you differently, others treat you differently in a different way, and yet others treat you the same. -->
<!-- This is because the function `DefinitionSentence` takes as its first argument a `Kind`, and ignores whether it's mass or count. -->

<!--
Thanks to the separation of abstract and concrete syntax, the ASTs don't need to include detail about articles. The subcategories of the Kinds play a role for some functions but not for others, but crucially, all of that detail is hidden inside the concrete syntax. -->


### When to do tree transformations

## Footnotes

<!-- [^1]: Although if you truly erase arguments, so that they contribute neither with textual content nor with a parameter, then nothing can save you from getting [metavariables](../../08/28/gf-gotchas.html#metavariables-or-those-question-marks-that-appear-when-parsing). -->

[^1]: The canonical source is [Shieber (1985)](https://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf), which presents examples from Swiss German. In certain constructions, constituents cross each other, as shown in the example below. Generalizing this phenomenon to an arbitrary number of nested clauses cannot be expressed with context-free grammars. <blockquote>![GF tree that demonstrates the word order in Swiss German subordinate clauses (the actual words are in English though.)](/images/swiss-german-subordinate-clause.png "Word order in Swiss German subordinate clauses")</blockquote>
[^2]: Finite approximations are possible for all of those languages whose description has an ‚Åø, and most applications have quite a small ‚Åø in practice.

[^3]: I believe that for most applications, if they are complex enough for you to use GF, you'd be better off using the RGL in the long run. But I also understand that the RGL has a learning curve, and you can get started faster if you define everything as strings. You can do quick RGL-free prototyping while you design your abstract syntax, not care if some linearizations are grammatically incorrect, and RGLify your concrete syntax once you're happy with the abstract.

[^4]: Mildly context-sensitive means "includes [these 3 features](https://en.wikipedia.org/wiki/Mildly_context-sensitive_grammar_formalism#Characterization) from the class of context-sensitive languages, plus all of context-free". And GF includes all of those + some more, details of which you can read from [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf).


[^5]: Suggestion for extending [the grammar](https://gist.github.com/inariksit/28e182bd4f6881cd69eb96121f048829): Refer to actions in form of a noun phrase, like "(the company's) **raising of capital**".<blockquote>![GF tree for 'the company's raising of capital is unethical'](/images/unethical.png "GF tree for 'the company's raising of capital is unethical'. Not present in the current grammar, implement this as an exercise!")</blockquote>
<!-- <p>The code is [here](https://gist.github.com/inariksit/28e182bd4f6881cd69eb96121f048829).</p> -->
<!-- Ref is short for reference, and it comes in a series with *predication*, *modification* and *reference*. TODO add table. -->


<!-- [^6]: In case you wonder: `WithPurpose` preserves the parameter of the `Kind`, so we get "a transaction" as well as "a transaction with the purpose of ‚Ä¶". The syntax <code><span class="token constant">WithPurpose</span> <span class="token hvariable">kind</span> <span class="token operator">=</span> <span class="token hvariable">kind</span> <span class="token operator"> ** </span> { s = ‚Ä¶ }</code> means that we <em>extend</em> the original argument with a new `s` field, but keep the rest of its fields (i.e. `c`) unchanged. -->

<!-- ```haskell
  WithPurpose kind = kind ** { -- Record extension
    -- only modify the s field here!
  }
```
-->

<!--
[^7]: The [previous a‚Åøb‚Åøc‚Åø tree](#context-sensitive-languages) was in fact also produced with a GF grammar! If you'd like to have a small exercise, try to recreate this tree.<br/> ![A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø](/images/aabbcc_alternative_tree.png "A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø") (Solution [here](https://gist.github.com/inariksit/f19f35c5ee46cef842757626cf81e630).) -->

[^8]: To give an example of the "world out there" that has **nothing to do with natural language**, here's a [popular science story](https://www.quantamagazine.org/amateur-mathematicians-find-fifth-busy-beaver-turing-machine-20240702/) about amateur mathematicians making a discovery in the theory of computation. It has cool illustrations of Turing machines, and it's written for non-expert audience.

<!-- [^9]: In case you're wondering what the abbreviations refer to: HPSG = [Head-driven phrase structure grammar](https://en.wikipedia.org/wiki/Head-driven_phrase_structure_grammar), LFG = [Lexical functional grammar](https://en.wikipedia.org/wiki/Lexical_functional_grammar), TAG = [Tree-adjoining grammar](https://en.wikipedia.org/wiki/Tree-adjoining_grammar) and CCG = [Combinatory categorial grammar](https://en.wikipedia.org/wiki/Combinatory_categorial_grammar). None of them is addressed further in this post, they are only included for illustrative purposes: GF is not alone in its niche at the border of context-free and context-sensitive languages. -->

[^9]: In case you're wondering what the abbreviations refer to: HPSG = Head-driven phrase structure grammar, LFG = Lexical functional grammar, TAG = Tree-adjoining grammar and CCG = Combinatory categorial grammar. None of them is addressed further in this post, they are only included for illustrative purposes: GF is not alone in its niche at the border of context-free and context-sensitive languages.