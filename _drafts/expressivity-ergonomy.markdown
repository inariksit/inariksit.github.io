---
layout: post
title:  "Expressivity and ergonomy of GF"
date:   2024-05-??
categories: gf
---

If you are reading this blog, you may have asked yourself the following question:

**Should I write my grammar in GF, or would I be fine with a more mainstream but less expressive formalism?**

This blog post aims to define what "is fine" means.

One of GF's selling points is its superior *expressivity* compared to context-free grammars, but the standard explanations for that sort of expressivity are either combinations of symbols that don't look even remotely like natural language, or Swiss German (or Dutch) triple subordinate clauses about promising to help someone paint their house. You are not making any such promises to anyone in any Germanic language, so is there a point in learning GF?

In order to answer that question, I will first define expressivity and then ergonomy, and

If you're looking for a rigorous treatise full of formal language theory, you're just one click away! Such a monograph has existed for 20 years already, so go ahead and read [LjunglÃ¶f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) if that's what you're after.


around 20% formal language theory and 80% software engineering.


## Expressivity of GF

It's not essential to understand every single detail of this section to appreciate the section about ergonomy. I start with a brief handwavy summary for those who are new to formal language theory, followed by a more exact definition for those who aren't.

### Gentle introduction

In this post, and in formal language theory in general, "language" means a set of strings generated by some grammar. We call these *generative grammars*, and they work so that you define set of rules, and by following the rules, you end up with some string. Here's a verbal description of a trivial grammar, which generates the language `{dog barks, cat barks, dog meows, cat meows}`.

```
Rule 0. You need to form a sentence.
Rule 1. Form a sentence: Concatenate one noun and one verb.
Rule 2. Nouns are: {dog, cat}.
Rule 3. Verbs are: {meows, barks}.
```

There are different ways of writing grammars, and with some of them, you can describe more complex phenomena than others. (What counts as "complex phenomenon" in formal language theory doesn't necessarily correspond to a language learner's intuitions of what is difficult!) The different kinds of grammars are divided into a *hierarchy* based on what kinds of languages they can express, and below is a simplified, handwavy picture that tries to give an intuition about the landscape.

![Handwave](/images/handwavy-explanation-chomsky-hierarchy.png "Simplified summary of Chomsky hierarchy and language complexity")


### Where is GF

GF is [equivalent to Parallel Multiple Context-Free Grammar](../../06/13/pmcfg.html#terminology) (PMCFG). It's between mildly context-sensitive and context-sensitive grammars. Writing a GF grammar for a non-context-free language like a^n^b^n^c^n^ is trivial, thanks to the lincats being *tuples of strings* instead of just strings.

So here's the GF grammar for a^n^b^n^c^n^.

```haskell
abstract ABC = {
  flags startcat = S ;
  cat
    S ; ABC ;
  fun
    mkS : ABC -> S ; -- concatenate all As, Bs, Cs into a single string
    addABC : ABC -> ABC ; -- add one A, one B, one C in right places
    zero : ABC ; -- n = 0, i.e. the empty string
}
```

And here is the concrete syntax for the grammar.

```haskell
concrete ABCcnc of ABC = {
  lincat
    S = Str ;
    ABC = {a,b,c : Str} ;
  lin
    mkS abc = abc.a ++ abc.b ++ abc.c ;
    addABC abc = {
      a = abc.a ++ "a" ;
      b = abc.b ++ "b" ;
      c = abc.c ++ "c"
      } ;
    zero = {a,b,c = ""} ;
}
```

You can test the grammar in your GF shell as follows:

```bash
$ gf ABCcnc.gf
â€¦
ABC> p "a a b b c c" | vp -view=open
```
![a2b2c2](/images/anbncn.png "GF parse tree for the string aabbcc")


You can also do stuff like erase[^1] and duplicate arguments on the right-hand side. If you are familiar with the [PGF format](../../06/13/pmcfg.html) and want to explore how this works on the lower level, you can add these functions to the previous grammars:

```haskell
fun
  eraseABC : ABC -> ABC ;
  duplicateABC : ABC -> ABC ;

lin
  eraseABC _ = {a,b,c = ""} ;
  duplicateABC abc = {
    a = abc.a ++ abc.a;
    b = abc.b ++ abc.b ;
    c = abc.c ++ abc.c
    } ;
```
and while in GF shell, you can print out the PGF with the command `pg` and observe the productions and sequences. But this is not at all necessary to follow the rest of this postâ€”I'm just leaving the option for anyone who might be interested in exploring more. And if you just now got interested in formal language theory and GF, here's the link to [LjunglÃ¶f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) again.


## Ergonomy of GF

As we learned in the previous section, GF being equivalent to PMCFG allows you to do some stuff that is literally impossible with a plain CFG.

But for most purposes, the relevant question to ask isn't "is it possible to represent __ with __". Often the answer is yes[^2], but it's still not a good idea.
1. There are plenty of things that are perfectly *possible*, but they are *painful* when all you have is a CFG.
2. It's not even that painful, but you want an AST that is robust for future changes in minor wordings.
3. You want multiple languages to have the same ASTs.

I feel like number 3 is the most commonly pitched thing about GF. But points 1 and 2 are relevant even if you only want to work on one language, even if that language is English, and even if you aren't even planning to use the RGL.[^3]

So now I will discuss how GF has [actually abstract ASTs](TODO), how it [makes impossible states unrepresentable](TODO) and how [I just love morphosyntax](TODO).

### Actually abstract ASTs

<!-- Coming from GF-land, it shocks me what kind of things people call ASTs. I see people writing grammars where *inflection forms of the same lexeme are in different functions*. ðŸ™€ -->

Consider the following two sentences:

- The company **raises capital**
- Equity financing means a transaction with the purpose of **raising capital**

The boldfaced fragments have different grammatical functions in their respective sentences. But semantically they express the same thing, and I would expect that to be reflected in the AST of an *application grammar*.

I have a small GF grammar that generates these sentences [here](TODO). (You are welcome to play around with the grammar, but it's not necessary to understand the examples on a high level.) Now look at the subtrees highlighted in blue in the following trees:

![GF tree for 'the company {raises capital}'](/images/predication.png "GF tree for 'the company raises capital'")

![GF tree for 'equity financing means a transaction with the purpose of {raising capital}'](/images/modification.png "GF tree for 'equity financing means a transaction with the purpose of raising capital'")

The subtree `Raise (IndefItem Capital)` has a different linearization depending on its context.
 contains an *inflection table*.


For bonus points, notice these subtrees:

```python
Startups> l IndefItem Transaction
a transaction

Startups> l IndefItem Capital
capital
```

We apply the same function `IndefItem` to two different `Kind`s, and get a different strategy: "a transaction" vs. "capital" without an article. This is because in the concrete syntax [StartupsEng](TODO), I have defined a parameter that specifies whether the `Kind` is a mass or a count noun. Then the linearization of `IndefItem` checks its argument's parameter, and inserts the indefinite article for count nouns, and no article for mass nouns.

This means that if you try to parse "a capital" or "transaction" in the category Item, it won't parse.

```python
Startups> p -cat=Item "a capital"
The parser failed at token 2: "capital"

Startups> p -cat=Item "transaction"
The parser failed at token 1: "transaction"
```

But in the context of *defining a term*, even count nouns appear without an article. Consider the following (rather tautological) trees and their linearizations:

```python
Startups> p "transaction means a transaction"
DefinitionSentence Transaction (IndefItem Transaction)

Startups> p "capital means capital"
DefinitionSentence Capital (IndefItem Capital)
```

You see that the function `DefinitionSentence` takes as its first argument a `Kind`


### Making impossible states unrepresentable

A good design principle is to make impossible states unrepresentable. I'm going to demonstrate this with a grammar that generates a generic transitive construction (*I see a cat*, *you see me*) and the reflexive construction (*I see myself*, *you see yourself*, â€¦).

Attempt 1 is a context-free grammar, and Attempt 2 is a GF grammar.

#### Attempt 1: overgenerating or overcomplicated

Let's start with a really quick and simple CFG.

```haskell
-- Version 0: overgenerating
S  := NP VP
VP := "see" NP
NP := "I" | "me" | "you" | "a cat" | "myself" | "yourself" | "itself"
```

This is really naive, and obviously wrong. It does generate all grammatically correct combinations, but also lots of junk, like "yourself see a cat".

```haskell
        S
      /   \
    NP     VP
    |     /  \
   "I" "see"  NP
  "you"       |
    â€¦      "a cat"
           "yourself"
           "myself"
           â€¦
```

Let's make a real attemptâ€”we can use [feature structures]() (easily available e.g. in NLTK) to limit the grammar a bit. We ignore agreement for now (e.g. *see* vs. *sees*) and only concentrate on the subject vs. object distinction. I split NP into two subclasses, called `NP[Subj]` and `NP[Obj]`.

```haskell
-- Version 1: subject vs. object separated
S        := NP[Subj] VP
VP       := "see" NP[Obj]
NP[Subj] := "I" | "you" | "a cat" |
NP[Obj]  := "me" | "you" | "a cat" | "myself" | "yourself" | "itself"
```

This version no longer generates "me" or "yourself" in subject position, but we can still get "I see ifself/yourself".

```haskell
          S
        /   \
      /       \
    NP[Subj]   VP
    |          /  \
   "I"      "see"  NP[Obj]
  "you"                |
 "a cat"            "a cat"
                    "yourself"
                    "myself"
                    â€¦
```
So let's do a first sketch on agreement (also added verb agreement):

```haskell
-- Version 2: first attempt at agreement
S           := NP[Subj,p] VP[p]
VP[p]       := V[p] NP[Obj,p]
NP[Subj,P1] := "I"
NP[Subj,P2] := "you"
NP[Subj,P3] := "a cat"
NP[Obj,P1]  := "me" | "myself"
NP[Obj,P2]  := "you" | "yourself"
NP[Obj,P3]  := "a cat" | "itself"
V[P1|P2]    := "see"
V[P3]       := "sees"
```

Now the grammar is too restrictiveâ€”it only allows "I see me/myself", "you see you/yourself" and "a cat sees a cat/itself". So we need to split the VPs even further: it's only the reflexive types of VP that have the stricter requirement. Here's a final version of this grammar:

```haskell
-- Version 3: reflexive VPs split into subclass
S           := NP[Subj,p] VP[p]
             | NP[Subj,p] ReflVP[p]
ReflVP[p]   := V[p] ReflNP[p]
VP[p]       := V[p] NP[Obj]
NP[Subj,P1] := "I"
NP[Subj,P2] := "you"
NP[Subj,P3] := "a cat"
NP[Obj]     := "me" | "you" | "a cat"
ReflNP[P1]  := "myself"
ReflNP[P2]  := "yourself"
ReflNP[P3]  := "itself"
V[P1|P2]    := "see"
V[P3]       := "sees"
```

This works, in that it generates all and only grammatically correct sentences, but the grammar is now much more complicated, with multiple subcategories. Yet it's frustratingly simplistic: "I", "me" and "myself" are all just terminals in the grammar, each of a different type.

#### Attempt 2: syncategorematicity to the rescue

Now let's write this grammar in GF. Of course, RGL users don't have to worry about this at all, but I believe this is an instructive example, so I will write it from scratch.

We start with an abstract syntax.

```haskell
abstract MiniGrammar = {
  cat
   S ; NP ; VP ;
   V2 ; -- V2 means /transitive verb/ = it takes an object
  fun
   Pred  : NP -> VP -> S ; -- Predication
   Compl : V2 -> NP -> VP ; -- Complementation
   Refl  : V2 -> VP ; -- Reflexive construction

   -- lexicon
   See : V2 ;
   I, You, ACat : NP ;
}
```

We're back to the simple grammatical categories, there are no subcategories *in the abstract syntax*. Also we have only a single entry for words that have multiple inflection forms: here called `I` and `See`, no *`Me` or *`Sees`.

The second thing I want to point out is the type of the reflexive construction. `Refl` only takes a single (transitive) verb as its argument, no explicit object! In fact, the reflexive pronouns don't even have independent lexical entries in the grammar. This is intentionalâ€”reflexive pronouns are not just any nominals, their use is restricted to a very particular construction. So it makes sense that they are only introduced in that construction and not elsewhere.

```
MiniGrammar> gt | l -tabtreebank
Pred ACat (Compl See ACat)    a cat sees a cat
Pred ACat (Compl See I)       a cat sees me
Pred ACat (Compl See You)     a cat sees you
Pred ACat (Refl See)          a cat sees itself
Pred I (Compl See ACat)       I see a cat
Pred I (Compl See I)          I see me
Pred I (Compl See You)        I see you
Pred I (Refl See)             I see myself
Pred You (Compl See ACat)     you see a cat
Pred You (Compl See I)        you see me
Pred You (Compl See You)      you see you
Pred You (Refl See)           you see yourself
```



## Footnotes

[^1]: Although if you truly erase arguments, so that they contribute neither with textual content nor with a parameter, then nothing can save you from getting [metavariables](../../08/28/gf-gotchas.html#metavariables-or-those-question-marks-that-appear-when-parsing).

[^2]: Finite approximations are possible for all of those languages whose description has an ^n, and most applications are finite.

[^3]: I believe that for most applications, if they are complex enough for you to use GF, you'd be better off using the RGL in the long run. But I also understand that the RGL has a learning curve, and you can get started faster if you define everything as strings. You can do quick RGL-free prototyping while you design your abstract syntax, not care if some linearizations are grammatically incorrect, and RGLify your concrete syntax once you're happy with the abstract.