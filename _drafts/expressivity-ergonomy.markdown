---
layout: post
title:  "Expressivity and ergonomy of GF"
date:   2024-05-??
categories: gf
---

If you are reading this blog, you may have asked yourself the following question:

**Should I write my grammar in GF, or would I be fine with a more mainstream but less expressive formalism?**

One of GF's selling points is its superior expressivity compared to its alternatives (that are often *context-free grammars*), but the standard explanations for that sort of expressivity are either combinations of symbols that don't look even remotely like natural language, or Swiss German triple subordinate clauses about promising to help someone paint their house. If neither of these sounds like your use case, is it worthwhile to learn GF?

In the first part of this post, I will first explain the concept of **expressivity**, focusing specifically on the spot that is interesting for natural language purposes.
In the second part of the post, I approach the question from the point of view of **ergonomy**.
I argue that even if it is *possible* express all of your domain with another tool, it might still be a more *pleasant* experience in GF.


<!-- The takeaway from this post will be as follows: Even if it is possible express all of your domain with another tool, it might still be a more pleasant experience in GF. -->




<!-- @import "[TOC]" {cmd="toc" depthFrom=1 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [Part I: Expressivity of GF](#part-i-expressivity-of-gf)
  - [What is expressivity: a gentle introduction](#what-is-expressivity-a-gentle-introduction)
    - [The Chomsky hierarchy: a simplified overview](#the-chomsky-hierarchy-a-simplified-overview)
    - [Regular languages](#regular-languages)
    - [Context-free languages](#context-free-languages)
    - [Context-sensitive languages](#context-sensitive-languages)
    - [Recursively enumerable languages](#recursively-enumerable-languages)
  - [Where is natural language](#where-is-natural-language)
  - [Where is GF](#where-is-gf)
- [Part II: Ergonomy of GF](#part-ii-ergonomy-of-gf)
  - [Actually abstract ASTs](#actually-abstract-asts)
    - [Inflection tables](#inflection-tables)
    - [Inherent parameters](#inherent-parameters)
  - [Making impossible states unrepresentable](#making-impossible-states-unrepresentable)
    - [Attempt 1: overgenerating or overcomplicated](#attempt-1-overgenerating-or-overcomplicated)
    - [Attempt 2: syncategorematicality to the rescue](#attempt-2-syncategorematicality-to-the-rescue)
- [Footnotes](#footnotes)

<!-- /code_chunk_output -->



## Part I: Expressivity of GF

It's not essential to understand every single detail of this section to appreciate the section about ergonomy. I start with a brief handwavy introduction for those who are new to formal language theory, followed by a more exact definition for those who aren't.

If you're looking for a rigorous treatise full of formal language theory, you're just one click away! Such a monograph has existed for 20 years already, so go ahead and read [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) if that's what you're after. Also if that's you, feel free to skip the whole gentle introduction, and jump directly to [Where is natural language](#where-is-natural-language).

### What is expressivity: a gentle introduction

In this post, and in formal language theory in general, "language" means a set of strings generated by some grammar. We call these *generative grammars*, and they work so that you define set of rules, and by following the rules, you end up with some string.

Here's a verbal description of a trivial grammar, which generates the language `{dog barks, cat barks, dog meows, cat meows}`.

> **Rule 1.** Concatenate one noun and one verb (in that order).  
> **Rule 2.** Nouns are: `{dog, cat}`.  
> **Rule 3.** Verbs are: `{meows, barks}`.  

There are different ways of writing grammars, and with some of them, you can describe more complex phenomena than others. (What counts as a "complex phenomenon" in formal language theory doesn't necessarily correspond to a language learner's intuitions of what is difficult!)
The different kinds of grammars are divided into a [*hierarchy*](https://en.wikipedia.org/wiki/Chomsky_hierarchy), named after its inventor Noam Chomsky.
 <!-- based on what kinds of languages they can express. -->


#### The Chomsky hierarchy: a simplified overview

Below is a simplified picture that tries to give an intuition about the landscape.

<!-- If you are already familiar with the Chomsky hierarchy, you can jump ahead to [Where is natural language](#where-is-natural-language). -->

![Simplified summary of Chomsky hierarchy and language complexity](/images/chomsky-hierarchy-handwavy-explanation.png "Simplified summary of Chomsky hierarchy and language complexity")

Now I'm going to shortly explain each level. Whenever I give an example of how one level is more expressive than the one before it, you may assume that the example is *representative*, but not *exhaustive*.

#### Regular languages

At the simplest level, we have **regular languages**. Our example string in the picture is the humble "aaaa‚Ä¶", which belongs to the language "any number of letter *a*".

What makes this language so simple? It's not because we only deal with a single character! We could've specified "any number of any Unicode characters in any order", and that language would've been a regular language‚Äîequally simple as our strings of plain *a*'s.

The simplicity comes from its (lack of) *tree structure* and (lack of) information about *what's happening elsewhere*.
<!--
 Here we have languages like "any number of letter *a*" or even "any number of *a*' followed by any number of *b*". But if you wanted to specify anything like "the same amount of *a* and *b*", that would be impossible: there is no way to remember what has come before the very latest token. -->

![A tree for the string 'aaaaaaaaaa'.](/images/aaaa.png "A tree for the string 'aaaaaaaaaa'.")

This tree is completely linear. The letters just come one after another, there is no more sophisticated structure. Furthermore, each letter in the string knows only what letter came immediately before it. This means that you would **not** be able to express anything that needs a longer "memory" (not a technical term), such as ~~"any amount of letter *a*, followed by the same amount of letter *b*"~~.

#### Context-free languages

Next in the hierarchy are **context-free languages**. Typical examples are markup languages or programming languages.

Compared to the regular languages, these languages have a slightly longer "memory". The HTML fragment in the [simplified overview](#the-chomsky-hierarchy-a-simplified-overview) is a good example: the `<a>` tag must be terminated with a corresponding `</a>` tag, and the complete `<a>‚Ä¶</a>` pair must be fully enclosed in the `<div> ‚Ä¶ </div>` tags.

For the sake of illustration, let's take the language that was impossible with regular grammars: "**any amount** of letter *a*, followed by **the same amount** of letter *b*". The conventional notation for this language is `a‚Åøb‚Åø`, which means "*n* repetitions of each letter."

Two things have changed from the sad-looking linear "trees" in the previous section.

![A tree for the string 'aaabbb' from the language a‚Åøb‚Åø](/images/aaabbb.png "A tree for the string 'aaabbb' from the language a‚Åøb‚Åø")

1. The trees have a non-linear, *nested* tree structure. (Intuition: they look more like üå≤ or üåø! Multiple things can grow from them simultaneously.) <!-- Real plants like üå≤ or üåø can grow new leaves in multiple places simultaneously, and so do context-free grammars. -->
2. In order to only produce strings that have exactly the right amounts of *a* and *b*, they need to come in pairs. You could replace any pair of (*a*,*b*) in the tree with a pair of opening and closing HTML tags. ![Nested HTML](/images/html.png "Nested HTML")

<!-- To allow for a larger lexicon (or *alphabet* as it's called in formal language theory), the grammar just needs more rules, but their idea is the same. Things can come in pairs and be nested

how many different characters we're dealing with is irrelevant! the full HTML specification has lots of different tags, but it's equally simple -->

However, if we wanted agreement between *more than two* elements, such as `a‚Åøb‚Åøc‚Åø`, that would no longer be a context-free language. Luckily, we have more levels of the Chomsky hierarchy!
<!-- (More on this language in the next section.) -->

<!-- We could pump out an "abc", and then surround it by more letters, but there's no way to insert those  -->

#### Context-sensitive languages

The next level is **context-sensitive languages**. There is no "typical example" of a context-sensitive language that would be familiar to non-experts. Contrast how HTML, an example of a context-free language, is known by many people who have never heard about the Chomsky hierarchy! But context-sensitive languages as a class don't have such a famous representative.

Previously, we saw that the language `a‚Åøb‚Åø` was context-free, and `a‚Åøb‚Åøc‚Åø` was not. But now we're in a more expressive realm, so we can generate that language! Here's a tree for the string "aabbcc".

![A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø](/images/aabbcc.png "A tree for the string 'aabbcc' from the language a‚Åøb‚Åøc‚Åø")

Look at that second *b* sneaking in between two letters! This would be impossible to do at a lower level. In a context-free language, you are allowed to output multiple letters at a time, but the moment you have laid them out on the paper, *their order may not change*, you can only add more letters before and after. In contrast, context-sensitive languages allow this kind of "late edits" (not a technical term).

<!-- They allow lots of other things too, but I want to keep this section short‚Äîthis is just to give some intuition. -->

I have a few things to say about natural language, but I'm saving it for the section [Where is natural language](#where-is-natural-language). So let's just finish climbing up the Chomsky hierarchy‚Äîonly one more level to go!

#### Recursively enumerable languages

The highest level is called **recursively enumerable languages**. Here I have used the set of prime numbers as an example language. This might look strange from a linguistic perspective: how come a list of numbers is "more complex" than a piece of legalese jargon?

The answer is that the grammar that generates the set of prime numbers has to actually *compute* the set. So it's not really a "grammar" in the same way as a book describing the morphosyntax of Basque or Greenlandic is a grammar. In formal language theory, grammars are actually *models of computation*.

The simpler ~~grammars~~ models of computation just deal with strings, but once we get to the top of the hierarchy, they can do whatever a Turing machine can. (If you don't know what a Turing machine is, just mentally replace it with "a computer".) As long as the answer to some computational problem has a textual representation, we can define a corresponding language to be *the set of all and only the strings that answer the problem*.


| Language | Grammar / Model of computation |
|----------|----------------------------------|
| All prime numbers | A Turing machine that computes prime numbers |
| (Singleton set of) the string that spells out the answer to life, universe and everything |  [Deep Thought](https://hitchhikers.fandom.com/wiki/Deep_Thought) |

If you thought this was interesting, you're lucky because there's a whole field out there, way beyond this blog and natural-language-oriented grammar formalisms!

If you thought this was a bunch of nonsense, you're lucky because ~~it won't be on the test~~ I have never seen any useful application of recursively enumerable languages in computational linguistics.

Speaking of which, now is finally the time to talk about natural language.

### Where is natural language

Computational linguists ignore most of the Chomsky hierarchy, and zoom in on a tiny dot, right at the border of context-free and context-sensitive languages. The picture below is still a gross oversimplification of all the different formalisms that have proliferated in that ecological niche.

![Chomsky hierarchy, zoomed in between CF and CS](/images/chomsky-hierarchy-zoomed-in.png "Chomsky hierarchy, zoomed in between CF and CS")

Many (computational) linguists agree that natural language goes beyond context-free[^1], but doesn't quite need the full power of context-sensitive languages. So people have set out to discover *subclasses* right between the two classes, each with different pros and cons. For these researchers, the tradeoff is expressivity of the formalism vs. a fast parsing algorithm.

Finally, a lot of people use plain old context-free grammars in order to model some *fragment* of natural language! This is a reasonable engineering decision, if you know that you only need to express a very simple domain. Context-free grammars are relatively fast, well-known, easy to use and very easy to find answers on Stack Overflow (especially compared to any of the formalisms named in the picture!) For a developer, the tradeoff is expressivity of the formalism vs. the learning curve to actually become productive.


<!-- The relevant landscape for natural languages is somewhere around context-free and context-sensitive.
![Chomsky hierarchy, only CF and CS](/images/chomsky-hierarchy-handwavy-explanation-cf-cs.png "Simplified summary of Chomsky hierarchy and language complexity, only focusing on natural language") -->

<!--
There is no scientific consensus on the classification of natural languages in the Chomsky hierarchy.

Even if there was one, it would still leave a lot of engineering questions,.

But for engineering purposes, there are plenty of empirical answers! There are different grammar formalisms with different features, and we can try them out to implement our applications.



  - Most grammar formalisms with somewhat mainstream adoption are somewhere between context-free (CF) and context-sensitive (CS).
  - In fact, formal language theorists have made up lots of subdivisions between the 4 classes! This is useful for performance purposes. The jump from CF to CS is pretty significant, and there are some things that are less relevant
  - For engineering purposes, there are plenty of usable fragments that can be expressed with context-free grammars.
- Some phenomena in some natural languages [aren't context-free](https://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf).
 -->



### Where is GF

GF is [equivalent to Parallel Multiple Context-Free Grammar](../../06/13/pmcfg.html#terminology) (PMCFG). Its expressive power is between *mildly context-sensitive* and *context-sensitive* languages[^4]‚Äîin a slot so tiny that it doesn't have its own name. This means that GF can express a *subset* of context-sensitive languages, but not all of them. However, it's a very useful subset for natural language purposes.

So let's demonstrate GF's expressive power by writing a GF grammar for the context-sensitive language `a‚Åøb‚Åøc‚Åø`. (The standard natural language examples are not nearly as compact.)
This is a simple task, thanks to the lincats being *tuples of strings* instead of just strings.

We start with the abstract syntax:

```haskell
abstract ABC = {
  flags startcat = S ;
  cat
    S ; ABC ;
  fun
    s : ABC -> S ; -- concatenate all As, Bs, Cs into a single string
    abc : ABC -> ABC ; -- add one A, one B, one C in right places
    empty : ABC ; -- n = 0, i.e. the empty string
}
```

And here is the concrete syntax for the grammar.

```haskell
concrete ABCcnc of ABC = {
  lincat
    S = Str ;
    ABC = {a,b,c : Str} ;
  lin
    s ABC = ABC.a ++ ABC.b ++ ABC.c ;
    abc ABC = {
      a = ABC.a ++ "a" ;
      b = ABC.b ++ "b" ;
      c = ABC.c ++ "c"
      } ;
    empty = {a,b,c = ""} ;
}
```

You can test the grammar in your GF shell as follows:

```bash
$ gf ABCcnc.gf
‚Ä¶
ABC> p "a a a b b b c c c"
s (abc (abc (abc empty)))
ABC> p "a a b b c c" | vp -showfun -view=open
```

![GF parse tree for the string aabbcc](/images/anbncn.png "GF parse tree for the string aabbcc")

This is just scratching the surface of formal language theory. If you want to learn about the larger context and how GF compares to some other grammar formalisms, here's the link to [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) again. But you don't *need* to read it to understand the rest of this post.


<!-- You can also do stuff like erase[^1] and duplicate arguments on the right-hand side. If you are familiar with the [PGF format](../../06/13/pmcfg.html) and want to explore how this works on the lower level, you can add these functions to the previous grammars:

```haskell
fun
  eraseABC : ABC -> ABC ;
  duplicateABC : ABC -> ABC ;

lin
  eraseABC _ = {a,b,c = ""} ;
  duplicateABC abc = {
    a = abc.a ++ abc.a;
    b = abc.b ++ abc.b ;
    c = abc.c ++ abc.c
    } ;
```
and while in GF shell, you can print out the PGF with the command `pg` and observe the productions and sequences. But this is not at all necessary to follow the rest of this post‚ÄîI'm just leaving the option for anyone who might be interested in exploring more. And if you just now got interested in formal language theory and GF, here's the link to [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf) again.
-->


## Part II: Ergonomy of GF

As we learned in the previous section, GF being equivalent to PMCFG allows you to do some stuff that is literally impossible with a plain CFG.

But for most purposes, the relevant question to ask isn't "is it possible to represent __ with __". Often the answer is yes[^2], but it's still not a good idea.

1. There are plenty of things that are perfectly *possible*, but they are *painful* when all you have is a CFG.
2. It might not be *that* painful, but you want an AST that is robust for future changes in minor wordings.
3. You want multiple languages to have the same ASTs.

I feel like number 3 is the most commonly pitched thing about GF. But points 1 and 2 are relevant even if you only want to work on one language, even if that language is English, and even if you aren't even planning to use the RGL.[^3]

So now I will discuss how GF has [actually abstract ASTs](#actually-abstract-asts), how it [makes impossible states unrepresentable](#making-impossible-states-unrepresentable) and how ‚Ä¶ (TODO more examples?).

### Actually abstract ASTs

<!-- Coming from GF-land, it shocks me what kind of things people call ASTs. I see people writing grammars where *inflection forms of the same lexeme are in different functions*. üôÄ -->

Consider the following two sentences:

- The company **raises capital**
- Equity financing means a transaction with the purpose of **raising capital**

The boldfaced fragments have different grammatical functions in their respective sentences. But semantically they express the same thing, and I would expect that to be reflected in the AST of an *application grammar*.

I have a small GF grammar that generates these sentences [here](https://gist.github.com/inariksit/28e182bd4f6881cd69eb96121f048829). (You are welcome to play around with the grammar, but it's not necessary to understand the examples on a high level.) Now look at the subtrees highlighted in blue in the following trees:

![GF tree for 'the company {raises capital}'](/images/predication.png "GF tree for 'the company raises capital'")

![GF tree for 'equity financing means a transaction with the purpose of {raising capital}'](/images/modification.png "GF tree for 'equity financing means a transaction with the purpose of raising capital'")

The subtree `Raise (IndefItem Capital)` has a different linearization depending on its context. When it's the main predicate, it linearizes to the finite verb form "raises capital". When it's modifying a noun as a part of an adverbial, it linearizes to a gerund "raising capital".

If you've ever written context-free grammars, I hope you can appreciate how much prettier this is to the alternative, where "raises" and "raising" would be completely disconnected productions!

The properties of GF that made it *possible* to implement a‚Åøb‚Åøc‚Åø are the same that make it *pretty* to implement this grammar. Let's discuss them one at a time.

#### Inflection tables

Previously, we saw that the lincat of `ABC` contained 3 different string fields, where we accumulated the appropriate amount of "a", "b" and "c" to be put together later. Now we make use of the concept "lincats may be tuples of strings" to create an *inflection table*.

TODO: GF code! Maybe also write a CFG version and link to it?

#### Inherent parameters

Let's look at this sentence again.

- Equity financing means **a transaction** with the purpose of raising **capital**

Did you notice that the function `IndefItem` was applied to both `Capital` and `Transaction`, but the indefinite article "a" was only linearized in one of them?

![GF tree for '{a transaction} with a purpose of raising {capital}'](/images/count_mass_nouns.png "GF tree for 'a transaction with a purpose of raising capital'")

<!--
```python
Startups> l IndefItem Transaction
a transaction

Startups> l IndefItem Capital
capital
```
We apply the same function `IndefItem` to two different `Kind`s, and get a different strategy: "a transaction" vs. "capital" without an article.
-->

This is possible, because in the concrete syntax [StartupsEng](TODO), I have defined an *inherent parameter* that specifies whether the `Kind` is a mass or a count noun. Then the linearization of `IndefItem` checks its argument's parameter, and inserts the indefinite article for count nouns, and no article for mass nouns.

This means that if you try to parse "a capital" or "transaction" in the category Item, it won't parse.

```python
Startups> p -cat=Item "a capital"
The parser failed at token 2: "capital"

Startups> p -cat=Item "transaction"
The parser failed at token 1: "transaction"
```

But in the context of *defining a term*, even count nouns appear without an article. Consider the following (rather tautological) trees and their linearizations:

```python
Startups> p "transaction means a transaction"
DefinitionSentence Transaction (IndefItem Transaction)

Startups> p "capital means capital"
DefinitionSentence Capital (IndefItem Capital)
```

You see that the function `DefinitionSentence` takes as its first argument a `Kind`


### Making impossible states unrepresentable

A good design principle is to make impossible states unrepresentable. I'm going to demonstrate this with a grammar that generates a generic transitive construction (*I see a cat*, *you see me*) and the reflexive construction (*I see myself*, *you see yourself*, ‚Ä¶).

Attempt 1 is a context-free grammar, and Attempt 2 is a GF grammar.

TODO: move the CFG to another place and only link to it.

#### Attempt 1: overgenerating or overcomplicated

Let's start with a really quick and simple CFG.

```haskell
-- Version 0: overgenerating
S  := NP VP
VP := "see" NP
NP := "I" | "me" | "you" | "a cat" | "myself" | "yourself" | "itself"
```

This is really naive, and obviously wrong. It does generate all grammatically correct combinations, but also lots of junk, like "yourself see a cat".

```haskell
        S
      /   \
    NP     VP
    |     /  \
   "I" "see"  NP
  "you"       |
    ‚Ä¶      "a cat"
           "yourself"
           "myself"
           ‚Ä¶
```

Let's make a real attempt‚Äîwe can use [feature structures]() (easily available e.g. in NLTK) to limit the grammar a bit. We ignore agreement for now (e.g. *see* vs. *sees*) and only concentrate on the subject vs. object distinction. I split NP into two subclasses, called `NP[Subj]` and `NP[Obj]`.

```haskell
-- Version 1: subject vs. object separated
S        := NP[Subj] VP
VP       := "see" NP[Obj]
NP[Subj] := "I" | "you" | "a cat" |
NP[Obj]  := "me" | "you" | "a cat" | "myself" | "yourself" | "itself"
```

This version no longer generates "me" or "yourself" in subject position, but we can still get "I see ifself/yourself".

```haskell
          S
        /   \
      /       \
    NP[Subj]   VP
    |          /  \
   "I"      "see"  NP[Obj]
  "you"                |
 "a cat"            "a cat"
                    "yourself"
                    "myself"
                    ‚Ä¶
```
So let's do a first sketch on agreement (also added verb agreement):

```haskell
-- Version 2: first attempt at agreement
S           := NP[Subj,p] VP[p]
VP[p]       := V[p] NP[Obj,p]
NP[Subj,P1] := "I"
NP[Subj,P2] := "you"
NP[Subj,P3] := "a cat"
NP[Obj,P1]  := "me" | "myself"
NP[Obj,P2]  := "you" | "yourself"
NP[Obj,P3]  := "a cat" | "itself"
V[P1|P2]    := "see"
V[P3]       := "sees"
```

Now the grammar is too restrictive‚Äîit only allows "I see me/myself", "you see you/yourself" and "a cat sees a cat/itself". So we need to split the VPs even further: it's only the reflexive types of VP that have the stricter requirement. Here's a final version of this grammar:

```haskell
-- Version 3: reflexive VPs split into subclass
S           := NP[Subj,p] VP[p]
             | NP[Subj,p] ReflVP[p]
ReflVP[p]   := V[p] ReflNP[p]
VP[p]       := V[p] NP[Obj]
NP[Subj,P1] := "I"
NP[Subj,P2] := "you"
NP[Subj,P3] := "a cat"
NP[Obj]     := "me" | "you" | "a cat"
ReflNP[P1]  := "myself"
ReflNP[P2]  := "yourself"
ReflNP[P3]  := "itself"
V[P1|P2]    := "see"
V[P3]       := "sees"
```

This works, in that it generates all and only grammatically correct sentences, but the grammar is now much more complicated, with multiple subcategories. Yet it's frustratingly simplistic: "I", "me" and "myself" are all just terminals in the grammar, each of a different type.

#### Attempt 2: syncategorematicality to the rescue

Now let's write this grammar in GF. Of course, RGL users don't have to worry about this at all, but I believe this is an instructive example, so I will write it from scratch.

We start with an abstract syntax.

```haskell
abstract MiniGrammar = {
  cat
   S ; NP ; VP ;
   V2 ; -- V2 means /transitive verb/ = it takes an object
  fun
   Pred  : NP -> VP -> S ; -- Predication
   Compl : V2 -> NP -> VP ; -- Complementation
   Refl  : V2 -> VP ; -- Reflexive construction

   -- lexicon
   See : V2 ;
   I, You, ACat : NP ;
}
```

We're back to the simple grammatical categories, there are no subcategories *in the abstract syntax*. Also we have only a single entry for words that have multiple inflection forms: here called `I` and `See`, no *`Me` or *`Sees`.

The second thing I want to point out is the type of the reflexive construction. `Refl` only takes a single (transitive) verb as its argument, no explicit object! In fact, the reflexive pronouns don't even have independent lexical entries in the grammar. This is intentional‚Äîreflexive pronouns are not just any nominals, their use is restricted to a very particular construction. So it makes sense that they are only introduced in that construction and not elsewhere.

```
MiniGrammar> gt | l -tabtreebank
Pred ACat (Compl See ACat)    a cat sees a cat
Pred ACat (Compl See I)       a cat sees me
Pred ACat (Compl See You)     a cat sees you
Pred ACat (Refl See)          a cat sees itself
Pred I (Compl See ACat)       I see a cat
Pred I (Compl See I)          I see me
Pred I (Compl See You)        I see you
Pred I (Refl See)             I see myself
Pred You (Compl See ACat)     you see a cat
Pred You (Compl See I)        you see me
Pred You (Compl See You)      you see you
Pred You (Refl See)           you see yourself
```



## Footnotes

<!-- [^1]: Although if you truly erase arguments, so that they contribute neither with textual content nor with a parameter, then nothing can save you from getting [metavariables](../../08/28/gf-gotchas.html#metavariables-or-those-question-marks-that-appear-when-parsing). -->

[^1]: The canonical source is [Shieber (1985)](https://www.eecs.harvard.edu/~shieber/Biblio/Papers/shieber85.pdf), with the classic painting-in-Swiss-German example.<br/>In case you're thinking "English isn't context-free because the inflection form of a verb depends on the subject", that's actually not what *context* means‚Äîsee [linguistics Stack Exchange](https://linguistics.stackexchange.com/questions/46893/why-isn-t-it-obvious-that-the-grammars-of-natural-languages-cannot-be-context-fr) for an answer.

[^2]: Finite approximations are possible for all of those languages whose description has an ‚Åø, and most applications are finite.

[^3]: I believe that for most applications, if they are complex enough for you to use GF, you'd be better off using the RGL in the long run. But I also understand that the RGL has a learning curve, and you can get started faster if you define everything as strings. You can do quick RGL-free prototyping while you design your abstract syntax, not care if some linearizations are grammatically incorrect, and RGLify your concrete syntax once you're happy with the abstract.

[^4]: Mildly context-sensitive means "includes [these 3 features](https://en.wikipedia.org/wiki/Mildly_context-sensitive_grammar_formalism#Characterization) from the class of context-sensitive languages, plus all of context-free". And GF includes all of those + some more, details of which you can read from [Ljungl√∂f (2004)](https://gupea.ub.gu.se/bitstream/handle/2077/16377/gupea_2077_16377_3.pdf).